{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM15s3qLfr5p"
   },
   "source": [
    "needed files for run:\n",
    "\n",
    "inverted_index_colab.py\n",
    "\n",
    "part15_preprocessed.pkl\n",
    "\n",
    "search_engine\n",
    "\n",
    "search_frontend\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
      "cluster-9906  GCE       2                                       RUNNING  us-central1-a\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tx6zjn-QxaNN",
    "outputId": "ca17991f-37e6-4df2-c634-4aa1ac54d431"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from timeit import timeit\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Jan 12 14:35 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-9906-m.c.finalproj3.internal:44029\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4d51cb5550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your bucket name below and make sure you can access it without an error\n",
    "bucket_name = 'buck_3_final' \n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if 'multistream' in b.name:\n",
    "        paths.append(full_path+b.name)\n",
    "\n",
    "# paths = paths[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yywsHEth3SKx"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import os\n",
    "\n",
    "# parquetFile = spark.read.parquet(*paths)\n",
    "title_docs = parquetFile.select(\"title\", \"id\").rdd.map(lambda e: (e['id'], e['title']))\n",
    "# body_docs = parquetFile.select(\"id\", \"text\").rdd.map(lambda r: (r[0],r[1]))\n",
    "# anchor_docs=parquetFile.select(\"id\", \"anchor_text\").rdd.map(lambda r: (r[0],r[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://id2title.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "| [1/1 files][168.9 MiB/168.9 MiB] 100% Done                                    \n",
      "Operation completed over 1 objects/168.9 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# convert id2title into dict pickle\n",
    "with open('id2title.pkl', 'wb') as f:\n",
    "    pickle.dump(dict(title_docs.collect()),f)\n",
    "\n",
    "!gsutil -m cp -r id2title.pkl gs://buck_3_final/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvYAxMDIOV2H"
   },
   "source": [
    "### Define funcs to build the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PtB1Ur09OcuZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘title_index’: File exists\n",
      "mkdir: cannot create directory ‘body_index’: File exists\n",
      "mkdir: cannot create directory ‘anchor_index’: File exists\n"
     ]
    }
   ],
   "source": [
    "%cd -q /home/dataproc\n",
    "!mkdir title_index\n",
    "!mkdir body_index\n",
    "!mkdir anchor_index\n",
    "\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token):\n",
    "  return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def word_count(text, id):\n",
    "    ''' Count the frequency of each word in `text` (tf) that is not included in \n",
    "    `all_stopwords` and return entries that will go into our posting lists. \n",
    "    Parameters:\n",
    "    -----------\n",
    "        text: str\n",
    "        Text of one document\n",
    "        id: int\n",
    "        Document id\n",
    "    Returns:\n",
    "    --------\n",
    "        List of tuples\n",
    "        A list of (token, (doc_id, tf)) pairs \n",
    "        for example: [(\"Anarchism\", (12, 5)), ...]\n",
    "    '''\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    counts = {}\n",
    "    for t in tokens:\n",
    "        if t not in all_stopwords:\n",
    "            counts[t] = counts.get(t, 0)+1\n",
    "    return [(w, (id,f)) for (w,f) in counts.items()]\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "  ''' Returns a sorted posting list by wiki_id.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    unsorted_pl: list of tuples\n",
    "      A list of (wiki_id, tf) tuples \n",
    "  Returns:\n",
    "  --------\n",
    "    list of tuples\n",
    "      A sorted posting list.\n",
    "  '''\n",
    "  return sorted(unsorted_pl, key=lambda e: e[0])\n",
    "\n",
    "def calculate_df(postings):\n",
    "  ''' Takes a posting list RDD and calculate the df for each token.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    postings: RDD\n",
    "      An RDD where each element is a (token, posting_list) pair.\n",
    "  Returns:\n",
    "  --------\n",
    "    RDD\n",
    "      An RDD where each element is a (token, df) pair.\n",
    "  '''\n",
    "  return postings.map(lambda e: (e[0], len(e[1])))\n",
    "\n",
    "def partition_postings_and_write(postings, base_dir):\n",
    "    ''' A function that partitions the posting lists into buckets, writes out \n",
    "    all posting lists in a bucket to disk, and returns the posting locations for \n",
    "    each bucket. Partitioning should be done through the use of `token2bucket` \n",
    "    above. Writing to disk should use the function  `write_a_posting_list`, a \n",
    "    static method implemented in inverted_index_colab.py under the InvertedIndex \n",
    "    class. \n",
    "    Parameters:\n",
    "    -----------\n",
    "        postings: RDD\n",
    "        An RDD where each item is a (w, posting_list) pair.\n",
    "    Returns:\n",
    "    --------\n",
    "        RDD\n",
    "        An RDD where each item is a posting locations dictionary for a bucket. The\n",
    "        posting locations maintain a list for each word of file locations and \n",
    "        offsets its posting list was written to. See `write_a_posting_list` for \n",
    "        more details.\n",
    "    '''\n",
    "\n",
    "    postings_buckets = postings.map(lambda e: (token2bucket_id(e[0]),e))\n",
    "    postings_buckets = postings_buckets.groupByKey()\n",
    "    return postings_buckets.map(lambda e: InvertedIndex.write_a_posting_list(e, base_dir))\n",
    "\n",
    "def merge_locs(posting_locs):\n",
    "    super_posting_locs_title = defaultdict(list)\n",
    "# for posting_loc in posting_locs_list_title:\n",
    "#   for k, v in posting_loc.items():\n",
    "#     super_posting_locs_title[k].extend(v)\n",
    "    for def_dict in posting_locs:\n",
    "        for term, locs in def_dict.items():\n",
    "            super_posting_locs_title[term].extend(locs)\n",
    "    return super_posting_locs_title\n",
    "\n",
    "def calc_term_total(postings):\n",
    "    s = 0\n",
    "    print(postings)\n",
    "    for x in postings[1]:\n",
    "        s += x[1]\n",
    "    return (postings[0], s)\n",
    "\n",
    "def calc_DL(text, id):\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n",
    "    return (id, len(tokens))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaSMQt0uQOlw"
   },
   "source": [
    "## build title index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ixzkoa7qQRup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://title_index.pkl [Content-Type=application/octet-stream]...\n",
      "- [1 files][ 90.6 MiB/ 90.6 MiB]                                                \n",
      "Operation completed over 1 objects/90.6 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "title_docs_pls = title_docs.flatMap(lambda e: word_count(e[1], e[0]))\n",
    "title_docs_pls = title_docs_pls.groupByKey().mapValues(reduce_word_counts)\n",
    "title_locs = partition_postings_and_write(title_docs_pls, 'buck_3_final').collect()\n",
    "\n",
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)\n",
    "\n",
    "title_df = calculate_df(title_docs_pls).collectAsMap()\n",
    "title_term_total = title_docs_pls.map(calc_term_total).collectAsMap()\n",
    "\n",
    "title_index = InvertedIndex()\n",
    "title_index.posting_locs = super_posting_locs\n",
    "title_index.df = title_df\n",
    "title_index.term_total = title_term_total\n",
    "title_index.write_index('.', 'title_index')\n",
    "\n",
    "index_src = \"title_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUgDncf9utA3"
   },
   "source": [
    "## build body index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FW5C4snourkg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://body_index.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1 files][  1.7 GiB/  1.7 GiB]  103.4 MiB/s                                   \n",
      "Operation completed over 1 objects/1.7 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "body_docs_pls = body_docs.flatMap(lambda e: word_count(e[1], e[0]))\n",
    "body_docs_pls = body_docs_pls.groupByKey().mapValues(reduce_word_counts)\n",
    "body_locs = partition_postings_and_write(body_docs_pls, bucket_name).collect()\n",
    "\n",
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)\n",
    "\n",
    "dl_body = body_docs.map(lambda x:calc_DL(x[1],x[0])).collectAsMap()\n",
    "body_df = calculate_df(body_docs_pls).collectAsMap()\n",
    "body_term_total = body_docs_pls.map(calc_term_total).collectAsMap()\n",
    "\n",
    "body_index = InvertedIndex()\n",
    "body_index.posting_locs = super_posting_locs\n",
    "body_index.df = body_df\n",
    "body_index.DL = dl_body\n",
    "body_index.term_total = body_term_total\n",
    "body_index.write_index('.', 'body_index')\n",
    "\n",
    "index_src = \"body_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoeOz1cMy2TG"
   },
   "source": [
    "## build anchor index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "I1G0SAtN-MfG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://anchor_index.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\ [1 files][166.6 MiB/166.6 MiB]                                                \n",
      "Operation completed over 1 objects/166.6 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "anchor=anchor_docs.flatMap(lambda x: x[1])\n",
    "anchor=anchor.distinct().groupByKey().mapValues(lambda x: \" \".join(x))\n",
    "anchor_pls = anchor.flatMap(lambda x: word_count(x[1], x[0]))\n",
    "anchor_pls = anchor_pls.groupByKey().mapValues(reduce_word_counts)\n",
    "anchor_locs = partition_postings_and_write(anchor_pls,bucket_name).collect()\n",
    "\n",
    "\n",
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)\n",
    "    \n",
    "dl_anchor=anchor.map(lambda x:calc_DL(x[1],x[0])).collectAsMap()\n",
    "anchor_df = calculate_df(anchor_pls).collectAsMap()\n",
    "anchor_term_total = anchor_pls.map(calc_term_total).collectAsMap()\n",
    "\n",
    "anchor_index = InvertedIndex()\n",
    "anchor_index.posting_locs = super_posting_locs\n",
    "anchor_index.df = anchor_df\n",
    "anchor_index.term_total = anchor_term_total\n",
    "anchor_index.DL = dl_anchor\n",
    "anchor_index.write_index('.', 'anchor_index')\n",
    "\n",
    "index_src = \"anchor_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### move indices to their relevant directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -m gs://buck_3_final/postings_gcp gs://buck_3_final/title_index\n",
    "!gsutil -m cp -r gs://buck_3_final/postings_gcp gs://buck_3_final/anchor_index\n",
    "!gsutil -m cp -r gs://buck_3_final/postings_gcp gs://buck_3_final/body_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e65QA2qU3ga8"
   },
   "source": [
    "## download indices from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ub3fMs4c3RFG",
    "outputId": "b274180b-ad87-4fed-9e2c-151468c2295d"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4204836408.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    get_ipython().system('gsutil -m cp -r gs://[BUCKET_NAME]/[FOLDER_NAME] [LOCAL_DIRECTORY]')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r gs://buck_3_final/title_index title_index\n",
    "!gsutil -m cp -r gs://buck_3_final/anchor_index anchor_index\n",
    "!gsutil -m cp -r gs://buck_3_final/body_index body_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCAc34VYrUoF"
   },
   "source": [
    "## Get page Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3bBhaduFrXup"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_unzip(r):\n",
    "    '''\n",
    "        take take row of the format [(id0,[id1,id2...])], and unzip\n",
    "        it to pairs, to the format [(id0,id1), (id0,id2), (id0,id3), ...] \n",
    "        this function is used as map function for the MapReduce\n",
    "    '''\n",
    "    return [(r[0],e) for e in r[1]]\n",
    "\n",
    "def generate_graph(pages):\n",
    "    ''' Compute the directed graph generated by wiki links.\n",
    "    Parameters:\n",
    "    -----------\n",
    "        pages: RDD\n",
    "        An RDD where each row consists of one wikipedia articles with 'id' and \n",
    "        'anchor_text'.\n",
    "    Returns:\n",
    "    --------\n",
    "        edges: RDD\n",
    "        An RDD where each row represents an edge in the directed graph created by\n",
    "        the wikipedia links. The first entry should the source page id and the \n",
    "        second entry is the destination page id. No duplicates should be present. \n",
    "        vertices: RDD\n",
    "        An RDD where each row represents a vetrix (node) in the directed graph \n",
    "        created by the wikipedia links. No duplicates should be present. \n",
    "    '''\n",
    "    # define indices\n",
    "    ID = 0\n",
    "    ANCHOR_TEXT = 1\n",
    "    pages = pages.map(lambda r: (r[ID], [int(x) for x in np.unique([i[ID] for i in r[ANCHOR_TEXT]])]))\n",
    "    # return pages\n",
    "    \n",
    "    vertices = pages.map(lambda e: [e[0]]+e[1])\n",
    "    vertices = vertices.flatMap(lambda e:e).distinct().map(lambda e:Row(id=e))\n",
    "    pages = pages.map(my_unzip)\n",
    "    \n",
    "    edges = pages.flatMap(lambda e:e)\n",
    "\n",
    "    return edges, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "jVZjrfsyrclq",
    "outputId": "647ea4a6-0200-4f49-934c-c71f6315fb3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "path hdfs://cluster-9906-m/user/root/pr already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m pr \u001b[38;5;241m=\u001b[39m pr_results\u001b[38;5;241m.\u001b[39mvertices\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpagerank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m pr \u001b[38;5;241m=\u001b[39m pr\u001b[38;5;241m.\u001b[39msort(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpagerank\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n\u001b[0;32m---> 10\u001b[0m \u001b[43mpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m pr\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1372\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression, sep\u001b[38;5;241m=\u001b[39msep, quote\u001b[38;5;241m=\u001b[39mquote, escape\u001b[38;5;241m=\u001b[39mescape, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   1366\u001b[0m                nullValue\u001b[38;5;241m=\u001b[39mnullValue, escapeQuotes\u001b[38;5;241m=\u001b[39mescapeQuotes, quoteAll\u001b[38;5;241m=\u001b[39mquoteAll,\n\u001b[1;32m   1367\u001b[0m                dateFormat\u001b[38;5;241m=\u001b[39mdateFormat, timestampFormat\u001b[38;5;241m=\u001b[39mtimestampFormat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1370\u001b[0m                charToEscapeQuoteEscaping\u001b[38;5;241m=\u001b[39mcharToEscapeQuoteEscaping,\n\u001b[1;32m   1371\u001b[0m                encoding\u001b[38;5;241m=\u001b[39mencoding, emptyValue\u001b[38;5;241m=\u001b[39memptyValue, lineSep\u001b[38;5;241m=\u001b[39mlineSep)\n\u001b[0;32m-> 1372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://cluster-9906-m/user/root/pr already exists."
     ]
    }
   ],
   "source": [
    "edges, vertices = generate_graph(anchor_docs)\n",
    "v_cnt, e_cnt = vertices.count(), edges.count()\n",
    "\n",
    "edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(4, 'id')\n",
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "pr = pr.sort(col('pagerank').desc())\n",
    "pr.repartition(1).write.csv('pr', compression=\"gzip\")\n",
    "pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move pageRank dict to cloud storage as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -get hdfs://cluster-9906-m/user/root/pr page_rank\n",
    "%cd -q page_rank\n",
    "!gunzip pageRank.csv.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pageRank.csv', index_col=0, names=['rank']).to_dict()['rank']  # {doc_id:rank}\n",
    "with open('pageRank.pkl', 'wb') as f:\n",
    "    pickle.dump(df,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://pageRank.pkl [Content-Type=application/octet-stream]...\n",
      "- [1/1 files][ 84.7 MiB/ 84.7 MiB] 100% Done                                    \n",
      "Operation completed over 1 objects/84.7 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r pageRank.pkl gs://buck_3_final/ "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
